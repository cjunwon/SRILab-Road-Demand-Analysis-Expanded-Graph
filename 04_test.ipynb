{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing intermediate_files/Node_to_Node_pairs.pickle...\n",
      "Importing intermediate_files/igraph.pickle...\n",
      "Importing intermediate_files/nodes_edges_ucla_big_graph.pickle...\n",
      "Importing intermediate_files/LODES_adjusted_block_pairing_count_list.pickle...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import igraph as ig\n",
    "import pickle\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "################### IMPORTANT SPECIFICATION TO CONTROL RUN SIZE ###################\n",
    "\n",
    "# There are len(Node_to_Node_pairs_dict) = 141593 origin nodes to compute.\n",
    "# You can specify how much to compute for each script run.\n",
    "\n",
    "START_INDEX = 100 # CHANGE AS NEEDED\n",
    "END_INDEX = 300 # CHANGE AS NEEDED\n",
    "# END_INDEX = 141593\n",
    "\n",
    "# To run all in one go, set START_INDEX = 0, END_INDEX = 141593\n",
    "\n",
    "# You can also specify the desired chunk size here.\n",
    "CHUNK_SIZE = 10\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "# ---------------------------------FILE IMPORT---------------------------------\n",
    "\n",
    "print(\"Importing intermediate_files/Node_to_Node_pairs.pickle...\")\n",
    "with open(r'intermediate_files/Node_to_Node_pairs.pickle', 'rb') as handle:\n",
    "    Node_to_Node_pairs = pickle.load(handle)\n",
    "\n",
    "print(\"Importing intermediate_files/igraph.pickle...\")\n",
    "with open(r'intermediate_files/igraph.pickle', 'rb') as handle:\n",
    "    g = pickle.load(handle)\n",
    "\n",
    "print(\"Importing intermediate_files/nodes_edges_ucla_big_graph.pickle...\")\n",
    "with open(r'nodes_edges_ucla_big_graph.pickle', 'rb') as handle:\n",
    "    B_matrix_sliced,B_matrix_str_sliced,nodes_coordinates_array = pickle.load(handle)\n",
    "\n",
    "print(\"Importing intermediate_files/LODES_adjusted_block_pairing_count_list.pickle...\")\n",
    "with open(r'intermediate_files/LODES_adjusted_block_pairing_count_list.pickle', 'rb') as handle:\n",
    "    Block_to_Block_Pairs, LODES_adjusted, block_pairing_count_list = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Node_to_Node_pairs_dict...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------PREPROCESSING---------------------------------\n",
    "\n",
    "\n",
    "\n",
    "expanded_LODES = [value for value, count in zip(LODES_adjusted, block_pairing_count_list) for _ in range(count)]\n",
    "expanded_LODES = np.array(expanded_LODES, dtype='d')\n",
    "\n",
    "\n",
    "lodes_lookup = {node_pair: lodes_val for node_pair, lodes_val in zip(Node_to_Node_pairs, expanded_LODES)}\n",
    "\n",
    "def get_lodes_from_node_pair(node_pair):\n",
    "    # Directly return the LODES_vals using the node_pair as the key\n",
    "    return lodes_lookup.get(node_pair, None)\n",
    "\n",
    "\n",
    "# Initialize array of 0s with 10 columns and append to B_matrix_sliced\n",
    "zero_columns = np.zeros((B_matrix_sliced.shape[0], 10))\n",
    "B_matrix_weighted_array = np.hstack((B_matrix_sliced, zero_columns))\n",
    "B_matrix_weighted_array = B_matrix_weighted_array.astype(float)\n",
    "\n",
    "\n",
    "# Converting B_matrix_weighted to a dictionary for faster lookups (O(1) lookups, faster)\n",
    "B_matrix_weighted_dict = {(row[0].astype(int), row[1].astype(int)): row for row in B_matrix_weighted_array}\n",
    "B_matrix_weighted_dict_blank = B_matrix_weighted_dict.copy()\n",
    "\n",
    "\n",
    "print(\"Creating Node_to_Node_pairs_dict...\")\n",
    "Node_to_Node_pairs_dict = defaultdict(list)\n",
    "\n",
    "for key, value in Node_to_Node_pairs:\n",
    "    Node_to_Node_pairs_dict[key].append(value)\n",
    "\n",
    "Node_to_Node_pairs_len = len(Node_to_Node_pairs)\n",
    "\n",
    "# ---------------------------------SUBSETTING & BATCH SETUP---------------------------------\n",
    "\n",
    "# count the total number of pairs in Node_to_Node_pairs_dict from keys and values\n",
    "total_subset_pairs = 0\n",
    "\n",
    "for key, value in Node_to_Node_pairs_dict.items():\n",
    "    total_subset_pairs += len(value)\n",
    "\n",
    "def chunks(data, CHUNK_SIZE=10):\n",
    "    #it = iter(data)\n",
    "    for i in range(0, len(data), CHUNK_SIZE):\n",
    "        it = iter(data)\n",
    "        if isinstance(data, dict):\n",
    "            yield {k: data[k] for k in list(it)[i:i + CHUNK_SIZE]}\n",
    "        else:\n",
    "            yield data[i:i + CHUNK_SIZE]\n",
    "\n",
    "Node_to_Node_pairs_subset_dict = dict(islice(Node_to_Node_pairs_dict.items(), START_INDEX, END_INDEX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using single source dijkstra algorithm in igraph to find shortest paths for  88813000 pairs...\n",
      "Creating chunks of Node_to_Node_pairs_dict_subset...\n",
      "Number of chunks to process:  20\n",
      "Processing chunk 1/20...\n",
      "Time taken to find shortest paths for chunk  1 :  0.2348514676094055 minutes\n",
      "Total time for B_matrix update: 25.507445812225342 seconds\n",
      "Time getting path and lodes values: 0.3714103698730469 seconds\n",
      "Time updating matrix: 25.127714157104492 seconds\n",
      "Processing chunk 2/20...\n",
      "Time taken to find shortest paths for chunk  2 :  0.020932018756866455 minutes\n",
      "Total time for B_matrix update: 0.3288838863372803 seconds\n",
      "Time getting path and lodes values: 0.030225038528442383 seconds\n",
      "Time updating matrix: 0.29668426513671875 seconds\n",
      "Processing chunk 3/20...\n",
      "Time taken to find shortest paths for chunk  3 :  0.02569490671157837 minutes\n",
      "Total time for B_matrix update: 0.8134119510650635 seconds\n",
      "Time getting path and lodes values: 0.036752939224243164 seconds\n",
      "Time updating matrix: 0.7755258083343506 seconds\n",
      "Processing chunk 4/20...\n",
      "Time taken to find shortest paths for chunk  4 :  0.04987396796544393 minutes\n",
      "Total time for B_matrix update: 7.295689821243286 seconds\n",
      "Time getting path and lodes values: 0.10251641273498535 seconds\n",
      "Time updating matrix: 7.189739227294922 seconds\n",
      "Processing chunk 5/20...\n",
      "Time taken to find shortest paths for chunk  5 :  0.08132632176081339 minutes\n",
      "Total time for B_matrix update: 11.554282903671265 seconds\n",
      "Time getting path and lodes values: 0.13750863075256348 seconds\n",
      "Time updating matrix: 11.395365715026855 seconds\n",
      "Processing chunk 6/20...\n",
      "Time taken to find shortest paths for chunk  6 :  0.052035049597422285 minutes\n",
      "Total time for B_matrix update: 1.5895838737487793 seconds\n",
      "Time getting path and lodes values: 0.0557093620300293 seconds\n",
      "Time updating matrix: 1.532649040222168 seconds\n",
      "Processing chunk 7/20...\n",
      "Time taken to find shortest paths for chunk  7 :  0.03834723234176636 minutes\n",
      "Total time for B_matrix update: 2.8962249755859375 seconds\n",
      "Time getting path and lodes values: 0.07006072998046875 seconds\n",
      "Time updating matrix: 2.824491262435913 seconds\n",
      "Processing chunk 8/20...\n",
      "Time taken to find shortest paths for chunk  8 :  0.0562533974647522 minutes\n",
      "Total time for B_matrix update: 3.1866860389709473 seconds\n",
      "Time getting path and lodes values: 0.04853963851928711 seconds\n",
      "Time updating matrix: 3.1360106468200684 seconds\n",
      "Processing chunk 9/20...\n",
      "Time taken to find shortest paths for chunk  9 :  0.031303465366363525 minutes\n",
      "Total time for B_matrix update: 1.440039873123169 seconds\n",
      "Time getting path and lodes values: 0.0271303653717041 seconds\n",
      "Time updating matrix: 1.412381887435913 seconds\n",
      "Processing chunk 10/20...\n",
      "Time taken to find shortest paths for chunk  10 :  0.04435969988505046 minutes\n",
      "Total time for B_matrix update: 3.1979100704193115 seconds\n",
      "Time getting path and lodes values: 0.05436992645263672 seconds\n",
      "Time updating matrix: 3.1420581340789795 seconds\n",
      "Processing chunk 11/20...\n",
      "Time taken to find shortest paths for chunk  11 :  0.01856890122095744 minutes\n",
      "Total time for B_matrix update: 0.3615250587463379 seconds\n",
      "Time getting path and lodes values: 0.009623289108276367 seconds\n",
      "Time updating matrix: 0.35163283348083496 seconds\n",
      "Processing chunk 12/20...\n",
      "Time taken to find shortest paths for chunk  12 :  0.015849681695302327 minutes\n",
      "Total time for B_matrix update: 0.3926858901977539 seconds\n",
      "Time getting path and lodes values: 0.008635759353637695 seconds\n",
      "Time updating matrix: 0.3837714195251465 seconds\n",
      "Processing chunk 13/20...\n",
      "Time taken to find shortest paths for chunk  13 :  0.024681615829467773 minutes\n",
      "Total time for B_matrix update: 6.46893310546875 seconds\n",
      "Time getting path and lodes values: 0.05558371543884277 seconds\n",
      "Time updating matrix: 6.411352634429932 seconds\n",
      "Processing chunk 14/20...\n",
      "Time taken to find shortest paths for chunk  14 :  0.024822346369425454 minutes\n",
      "Total time for B_matrix update: 2.8018240928649902 seconds\n",
      "Time getting path and lodes values: 0.04794430732727051 seconds\n",
      "Time updating matrix: 2.752385139465332 seconds\n",
      "Processing chunk 15/20...\n",
      "Time taken to find shortest paths for chunk  15 :  0.03797092835108439 minutes\n",
      "Total time for B_matrix update: 4.27931809425354 seconds\n",
      "Time getting path and lodes values: 0.04633760452270508 seconds\n",
      "Time updating matrix: 4.231071710586548 seconds\n",
      "Processing chunk 16/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/88dwrhnx7m3cybxwl1p0rtq40000gn/T/ipykernel_72701/2127745129.py:22: RuntimeWarning: Couldn't reach some vertices. at src/paths/dijkstra.c:534\n",
      "  shortest_paths = g.get_shortest_paths(origin, to=destinations, weights='weight', output=\"vpath\", algorithm=\"dijkstra\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to find shortest paths for chunk  16 :  0.03134719928105672 minutes\n",
      "Total time for B_matrix update: 4.378596067428589 seconds\n",
      "Time getting path and lodes values: 0.3897278308868408 seconds\n",
      "Time updating matrix: 3.9862239360809326 seconds\n",
      "Processing chunk 17/20...\n",
      "Time taken to find shortest paths for chunk  17 :  0.03222777048746745 minutes\n",
      "Total time for B_matrix update: 2.18742299079895 seconds\n",
      "Time getting path and lodes values: 0.044085025787353516 seconds\n",
      "Time updating matrix: 2.1420094966888428 seconds\n",
      "Processing chunk 18/20...\n",
      "Time taken to find shortest paths for chunk  18 :  0.04871776501337687 minutes\n",
      "Total time for B_matrix update: 19.82479977607727 seconds\n",
      "Time getting path and lodes values: 0.1459193229675293 seconds\n",
      "Time updating matrix: 19.671318292617798 seconds\n",
      "Processing chunk 19/20...\n",
      "Time taken to find shortest paths for chunk  19 :  0.04166420300801595 minutes\n",
      "Total time for B_matrix update: 14.579031229019165 seconds\n",
      "Time getting path and lodes values: 0.08786988258361816 seconds\n",
      "Time updating matrix: 14.4868643283844 seconds\n",
      "Processing chunk 20/20...\n",
      "Time taken to find shortest paths for chunk  20 :  0.04747788508733113 minutes\n",
      "Total time for B_matrix update: 11.564475774765015 seconds\n",
      "Time getting path and lodes values: 0.06709504127502441 seconds\n",
      "Time updating matrix: 11.493646144866943 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------MAIN ALGORITHM & B_MATRIX UPDATE---------------------------------\n",
    "\n",
    "\n",
    "print(\"Using single source dijkstra algorithm in igraph to find shortest paths for \", total_subset_pairs, \"pairs...\")\n",
    "\n",
    "print(\"Creating chunks of Node_to_Node_pairs_dict_subset...\")\n",
    "Node_to_Node_pairs_subset_dict_chunks = list(chunks(Node_to_Node_pairs_subset_dict, CHUNK_SIZE))\n",
    "\n",
    "print(\"Number of chunks to process: \", len(Node_to_Node_pairs_subset_dict_chunks))\n",
    "\n",
    "missing_paths = 0\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "for i, chunk in enumerate(Node_to_Node_pairs_subset_dict_chunks):\n",
    "\n",
    "    chunk_time_start = time.time()\n",
    "\n",
    "    shortest_path_results = {}\n",
    "    print(f\"Processing chunk {i+1}/{len(Node_to_Node_pairs_subset_dict_chunks)}...\")\n",
    "    for origin, destinations in chunk.items():\n",
    "        shortest_paths = g.get_shortest_paths(origin, to=destinations, weights='weight', output=\"vpath\", algorithm=\"dijkstra\")\n",
    "        to_destination_dict = {}\n",
    "        for destination, path in zip(destinations, shortest_paths):\n",
    "            if len(path) == 0:\n",
    "                missing_paths += 1\n",
    "            else:\n",
    "                to_destination_dict[destination] = path\n",
    "\n",
    "        shortest_path_results[origin] = to_destination_dict\n",
    "\n",
    "    shortest_path_tuple_keys = {(origin, destination): path \n",
    "                            for origin, destinations in shortest_path_results.items() \n",
    "                            for destination, path in destinations.items()}\n",
    "\n",
    "    chunk_time_end = time.time()\n",
    "\n",
    "    print(\"Time taken to find shortest paths for chunk \", i+1, \": \", (chunk_time_end - chunk_time_start)/60, \"minutes\")\n",
    "\n",
    "    B_matrix_update_time_start = time.time()\n",
    "\n",
    "    # Initialize variables to track time spent in different parts of the loop\n",
    "    time_getting_path_and_lodes = 0\n",
    "    time_updating_matrix = 0\n",
    "\n",
    "    for key in shortest_path_tuple_keys:\n",
    "        start_time = time.time()\n",
    "        path = shortest_path_tuple_keys[key]\n",
    "        lodes_values = get_lodes_from_node_pair(key)\n",
    "        time_getting_path_and_lodes += time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        for j in range(len(path) - 1):\n",
    "            pair = (path[j], path[j+1])\n",
    "            reverse_pair = (path[j+1], path[j])\n",
    "\n",
    "            if pair in B_matrix_weighted_dict:\n",
    "                B_matrix_weighted_dict[pair][6:16] += lodes_values\n",
    "            elif reverse_pair in B_matrix_weighted_dict:\n",
    "                B_matrix_weighted_dict[reverse_pair][6:16] += lodes_values\n",
    "        \n",
    "        time_updating_matrix += time.time() - start_time\n",
    "\n",
    "    B_matrix_update_time_end = time.time()\n",
    "\n",
    "    # print(\"Time taken to update B_matrix_weighted_array for chunk \", i+1, \": \", (B_matrix_update_time_end - B_matrix_update_time_start)/60, \"minutes\")\n",
    "    # Print the total time and the time spent in each part\n",
    "    print(f\"Total time for B_matrix update: {B_matrix_update_time_end - B_matrix_update_time_start} seconds\")\n",
    "    print(f\"Time getting path and lodes values: {time_getting_path_and_lodes} seconds\")\n",
    "    print(f\"Time updating matrix: {time_updating_matrix} seconds\")\n",
    "\n",
    "    del shortest_path_tuple_keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting B_matrix_weighted_dict back to B_matrix_weighted_array...\n",
      "Percent of B_matrix_weighted_array updated:  58.474643805337564 %\n",
      "Time taken to convert B_matrix_weighted_dict back to B_matrix_weighted_array:  0.005560032526652018 minutes\n",
      "Time taken for all chunks:  3.0446722507476807 minutes\n",
      "Estimated time for all pairs:  0.050744537512461343 hours\n",
      "Number of missing paths:  84\n",
      "Percentage of missing paths: 9.458074831387296e-05%\n",
      "Exporting B_matrix_weighted_dict to intermediate_files/B_matrix_weighted_dict.pickle...\n",
      "Exporting B_matrix_weighted_array to intermediate_files/B_matrix_weighted_array.pickle...\n"
     ]
    }
   ],
   "source": [
    "# convert B_matrix_weighted_dict back to B_matrix_weighted_array\n",
    "\n",
    "print(\"Converting B_matrix_weighted_dict back to B_matrix_weighted_array...\")\n",
    "\n",
    "convert_start = time.time()\n",
    "\n",
    "del B_matrix_weighted_array\n",
    "\n",
    "# use keys and values from B_matrix_weighted_dict to create B_matrix_weighted_array\n",
    "\n",
    "B_matrix_weighted_array = np.array(list(B_matrix_weighted_dict.values()))\n",
    "\n",
    "# find number of rows in B_matrix_weighted_array where the last 10 columns are not 0\n",
    "non_zero_rows = np.where(B_matrix_weighted_array[:,-10:].any(axis=1))[0]\n",
    "print(\"Percent of B_matrix_weighted_array updated: \", len(non_zero_rows)/B_matrix_weighted_array.shape[0] * 100, \"%\")\n",
    "\n",
    "convert_end = time.time()\n",
    "\n",
    "print(\"Time taken to convert B_matrix_weighted_dict back to B_matrix_weighted_array: \", (convert_end - convert_start)/60, \"minutes\")\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "print(\"Time taken for all chunks: \", (time_end - time_start)/60, \"minutes\")\n",
    "print(\"Estimated time for all pairs: \", (time_end - time_start)/60 * Node_to_Node_pairs_len/total_subset_pairs / 60, \"hours\")\n",
    "\n",
    "print(\"Number of missing paths: \", missing_paths)\n",
    "percentage_missing_paths = missing_paths/total_subset_pairs * 100\n",
    "print(\"Percentage of missing paths: \" + str(percentage_missing_paths) + \"%\")\n",
    "\n",
    "\n",
    "# ---------------------------------OUTPUT EXPORT---------------------------------\n",
    "\n",
    "# Export B_matrix_weighted_dict to a pickle file\n",
    "\n",
    "filename = f\"B_matrix_weighted_dict_{START_INDEX}_to_{END_INDEX-1}.pickle\"\n",
    "\n",
    "print(\"Exporting B_matrix_weighted_dict to intermediate_files/B_matrix_weighted_dict.pickle...\")\n",
    "\n",
    "with open(f'intermediate_files/{filename}', 'wb') as handle:\n",
    "    pickle.dump(B_matrix_weighted_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Export B_matrix_weighted_array to a pickle file\n",
    "\n",
    "filename = f\"B_matrix_weighted_array_{START_INDEX}_to_{END_INDEX-1}.pickle\"\n",
    "\n",
    "print(\"Exporting B_matrix_weighted_array to intermediate_files/B_matrix_weighted_array.pickle...\")\n",
    "with open(f'intermediate_files/{filename}', 'wb') as handle:\n",
    "    pickle.dump(B_matrix_weighted_array, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
